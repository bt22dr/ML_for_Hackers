---
title: "회귀 모형"
output: html_document
---

### 회귀
* 어떤 숫자 집합(input, predictor, feature)으로 다른 숫자 집합(output)을 예측하는 것
* 분류와 다른 점은 출력값이 실제 숫자라는 점
* ex) 흡연 습관을 바탕으로 수명 예측, 전날의 기온을 기초로 다음날 기온 예상 
* 통계학자들이 지난 200년간 연구한 다양한 회귀 알고리즘들은 모두 입력값을 출력값으로 바꾸어 예측을 만드는 방법론들이다. 

#### 선형 회귀의 기본 모형
우리가 가진 정보를 입력값으로 활용하는 가장 간단한 방법?  
-> 입력값 없이 과거의 출력값의 평균만으로 미래를 예측하는 방법  
(만약 아무런 정보가 없다면 가장 참 값에 가까운 최적 예측값은 평균값으로 알려져 있다. 뒤에서 자세히 설명)

**- 흡연자와 비흡연자를 비교하는 밀도 그래프**
```{r}
library('ggplot2')

# First snippet
ages <- read.csv(file.path('data', 'longevity.csv'))

# 각 분포의 중심 위치가 다르므로
# 흡연과 수명이 상관이 있다고 보는 게 합리적이다. 
ggplot(ages, aes(x = AgeAtDeath, fill = factor(Smokes))) +
  geom_density() +
  facet_grid(Smokes ~ .)
```

우리에게 이런 정보가 없다고 가정해보자.  
만약 흡연 습관 정보 없이 수명을 예측하는 숫자를 하나 골라야 한다면?  
-> 제곱오차(squared error, ( 참 값(y) - y에 대한 가설(h) )^2) 측도가 작은 것이 좋은 예측 

앞에서 본 데이터에서 평균 수명(AgeAtDeath)은 `r mean(ages$AgeAtDeath)`이다.  
예측값을 73정도로 추정할 경우 예측 성능이 얼마나 나빠지는가? or 측도값이 얼마인가?
```{r} 
ages <- read.csv(file.path('data', 'longevity.csv'))
guess <- 73
(mse <- with(ages, mean((AgeAtDeath - guess) ^ 2)))
```
평균 제곱 오차는 `r mse`. 이것이 최적 추정치인지 확인하기 위해 63, 83 범위에서 mse값을 모두 계산해보자.
```{r}
ages <- read.csv(file.path('data', 'longevity.csv'))

guess.accuracy <- data.frame()

for (guess in seq(63, 83, by = 1))
{
  prediction.error <- with(ages,
                           mean((AgeAtDeath - guess) ^ 2))
  guess.accuracy <- rbind(guess.accuracy,
                          data.frame(Guess = guess,
                                     Error = prediction.error))
}

ggplot(guess.accuracy, aes(x = Guess, y = Error)) +
  geom_point() +
  geom_line()
```

추정치 73 부근에서 최저점을 보인다. 일반적으로 제곱오차를 최소화하는 예측값은 평균값임이 수학적으로 증명되어 있다.  
-> 중요한 사실을 알려줌 : 흡연에 대한 정보를 추가해서 추정한 값이 단순 평균값을 써서 추정했을 때보다 얼마나 더 성능이 좋은지를 바탕으로 평가해야 한다. 즉, 단순 평균값을 이용해 구한 값이 일종의 기준점이 된다. 

#### 가변수를 활용한 회귀
사람들의 흡연 여부 정보를 어떻게 써야 수명을 더 잘 예측할 수 있을까?  
-> 흡연자와 비흡연자의 평균 사망 나이를 따로 계산, 각각의 예측값으로 활용

**-흡연자와 비흡연자 두 집단의 각 평균값으로 예측값을 사용하여 RMSE 계산**
```{r}
ages <- read.csv(file.path('data', 'longevity.csv'))

constant.guess <- with(ages, mean(AgeAtDeath))

(constant.rmse <- with(ages, sqrt(mean((AgeAtDeath - constant.guess) ^ 2))))

smokers.guess <- with(subset(ages, Smokes == 1),
                      mean(AgeAtDeath))

non.smokers.guess <- with(subset(ages, Smokes == 0),
                          mean(AgeAtDeath))

ages <- transform(ages,
                  NewPrediction = ifelse(Smokes == 0,
                                         non.smokers.guess,
                                         smokers.guess))

(smoke.rmse <- with(ages, sqrt(mean((AgeAtDeath - NewPrediction) ^ 2))))
```

결과는 아래 표와 같다. 정보를 추가한 경우 예측 오차가 약 10% 감소했다. 이처럼 데이터 점을 둘로 분류할 수 있을 때, 두 개의 종류가 예측하려는 출력값과 연관이 있는 경우에는 정보를 추가했을 때 일반적으로 더 나은 결과를 낸다. ex) 남자/여자, 공화당/민주당

정보 | 평균제곱근오차
-----|---------------
흡연정보 X | `r constant.rmse`
흡연정보 O | `r smoke.rmse`

추가로, 정보가 많아진다는 이야기는 두 가지를 뜻한다. 

* 이항 분류 대신 연속적인 값을 입력값으로 사용
* 여러 정보를 동시에 사용. ex) 흡연 여부와 부모의 사망 연령 정보를 모두 사용


#### 선형회귀 
가지고 있는 여러 정보를 다 활용하기란 쉽지 않다. 실제로 정보를 모두 활용하려면 가정을 좀 단순화할 필요가 있다. 선형 회귀에서 사용하는 가정 두 가지는 아래와 같다.

* 분리성(separability)/가법성(additivity)  
추정치에 영향을 주는 정보가 여러 개라면 각 정보를 독립적으로 활용했을 때처럼 각각의 효과를 더하기가 가능하다고 가정한다. 예를 들어, 알콜중독 1년, 흡연자 5년 일때 알콜중독 흡연자는 6년이 된다는 뜻. 각 경우가 동시에 발생했을 때 각 효과가 독립적으로 더해진다는 가정은 매우 큰 가정이지만 회귀 분석을 응용할 때 좋은 시작접이 되는 경우가 많다. 
* 단조성(monotonicity)/선형성(linearity)  
모형의 입력 중 하나를 변화시키면 예측 출력이 항상 증가하거나 감소하는 성질을 단조성이라고 한다. 단조성 가정도 강하긴 하지만 선형 회귀의 기본 가정인 선형성보다는 훨씬 약하다. 선형 모형은 항상 단조성 모형이지만, 선형이 아니어도 단조성인 곡선은 존재할 수 있다. (참고: Curve, Line, Wave 그림)

**- 선형 회귀 예제**

가법성과 선형성을 염두에 둔 채로 간단한 선형 회귀 예제를 다뤄 보자.
```{r}
library('ggplot2')

# 키와 몸무게 데이터
heights.weights <- read.csv(file.path('data',
                                      '01_heights_weights_genders.csv'),
                            header = TRUE,
                            sep = ',')
# geom_smooth에 선형 모델을 뜻하는 lm 인수를 설정하고 호출하여 선형회귀 직선을 그려준다.
ggplot(heights.weights, aes(x = Height, y = Weight)) +
  geom_point() +
  geom_smooth(method = 'lm')
```

직선을 이용해서 키로 몸무게를 예측하는 방법이 꽤 잘 동작함을 알 수 있다.  
그러면 어떻게 이 그래프의 직선을 정의하는 숫자를 찾을 수 있을까?  
lm을 실행하고 나면, 이력과 출력 사이의 선형 모형 계수를 반환하는 coef 함수를 이용해 회귀선의 절편과 기울기를 알아낼 수 있다. (선형 모형 : 2차원에서 직선, 3차원에서 평면, 그 이상에서는 초평면)

```{r}
fitted.regression <- lm(Weight ~ Height,
                        data = heights.weights)

coef(fitted.regression)

intercept <- coef(fitted.regression)[1]
slope <- coef(fitted.regression)[2]

# coef가 반환한 값은 아래와 같이 해석하면 된다. 
# predicted.weight <- intercept + slope * observed.height
# predicted.weight == -350.737192 + 7.717288 * observed.height
```

그런데 위 선형회귀식에 따르면 몸무게가 0 파운드가 되려면 키가 45인치라야 한다.  
이 회귀 모형은 어린이나 키가 아주 작은 어른에게는 잘 맞지 않는다. 이것은 선형 회귀상의 일반적인 문제로,  
예측 모형은 보통 과거에 관찰했던 입력값들에서 크게 동떨어져 있는 값에 대해서는 성능이 별로 좋지 않다. 
(이런 현상을 두고 기술적 용어로, 회귀는 내삽(interpolation) 성능은 좋지만 외삽(extrapolation)은 좋지 않다고 말한다)

**- 선형 회귀의 결과를 확인하는 방법들**

모형의 예측값을 predict 함수를 이용해 얻고 참 값과의 차이를 계산한다.  
이렇게 계산한 오차는 회귀선이 설명하고 남은 부분이라는 이유로 잔차(residual)라고 불린다.  
residual 함수를 써서 잔차를 구하고 그래프로 표현해본다.  
```{r}
head(predict(fitted.regression), 30)
```

```{r}
true.values <- with(heights.weights, Weight)
errors <- true.values - predict(fitted.regression)

head(residuals(fitted.regression), 30)

plot(fitted.regression, which = 1)
```

-> TODO: 빨간 선의 의미를 찾아보기  
잔차에 체계적 구조가 없으므로 선형 모형이 잘 적용된다고 말할 수 있다.  
하지만 아래와 같은 데이터의 경우에는 잔차에서 뚜렷한 구조가 보인다. 
```{r echo=FALSE}
x <- 1:10
y <- x ^ 2

fitted.regression <- lm(y ~ x)
plot(fitted.regression, which = 1)
```

모형에서는 관찰 대상이 신호(predict 함수의 결과)와 잡음(residuals 함수의 결과)로 나뉘어야 한다.  
육안으로도 잔차에서 신호가 보인다면, 그 모형은 신호를 다 잡아내서 잔차에 잡음만 남길만큼 강력하지 않다는 뜻이다.  

**- 회귀 모형의 평가**

잔차 -> 제곱오차(SSE) -> 평균제곱오차(MSE) -> 평균제곱근오차(RMSE)
```{r}
x <- 1:10
y <- x ^ 2

fitted.regression <- lm(y ~ x)

errors <- residuals(fitted.regression)
squared.errors <- errors ^ 2
(sse <- sum(squared.errors))
(mse <- mean(squared.errors))
(rmse <- sqrt(mse))
```

RMSE의 단점은 썩 좋지 않은 성능이 무언지 바로 명확하게 와닿지 않는다는 것.  
RMSE 값은 제한이 없기 때문에 잘못된 값을 예측하면 계속해서 값이 커질 수 있기 때문이다.  
-> 해결책은 R^2 값을 사용하면 된다. 위에서 사용한 모델의 R-squared 값은 다음과 같이 구할 수 있다.
```{r}
summary(fitted.regression)$r.squared
cor(x, y)^2
```
R-squared는 평균 출력값만 이용하는 예측 모형의 MSE와 해당 모형의 MSE의 비율로 구하며  
두 변수의 상관계수의 제곱값과 동일하다.



########################

### 웹 트래픽 예측
회귀를 이용해서 2011년 인터넷 상위 1000개 웹사이트의 페이지 방문 수를 예측하는 예제를 살펴보자. Neil Kodner가 제공한 데이터셋에서 Rank(순위), PageViews(페이지 방문 수), UniqueVisitors(순방문자 수), HasAdvertising(광고 포함 여부), IsEnglish(영문 여부)만 사용한다. (표 5-3)

PageViews와 UniqueVisitors의 관계를 산포도로 그려보자.

```{r}
top.1000.sites <- read.csv(file.path('data', 'top_1000_sites.tsv'),
                           sep = '\t',
                           stringsAsFactors = FALSE)

ggplot(top.1000.sites, aes(x = PageViews, y = UniqueVisitors)) +
  geom_point()
```

심각하다. 이런 문제는 정규 분포가 아닌 데이터에서 자주 나타나는데, 전체 값들을 다 보여줄 만큼 큰 척도를 사용하면 데이터 점 대다수가 가깝게 위치해서 눈으로는 구별이 안된다. 

PageViews만의 분포를 살펴보자.
```{r}
ggplot(top.1000.sites, aes(x = PageViews)) +
  geom_density()

ggplot(top.1000.sites, aes(x = log(PageViews))) +
  geom_density()
```

PageViews만 봐도 값이 몰려 있는 것을 알 수 있다. 분석하려는 값에 로그를 취한 값으로 밀도 그래프를 다시 그려보면 데이터를 파악하는데 훨씬 도움이 된다.

두 변수를 모두 로그변환하고 회귀식을 그려보면 아래와 같다. 
```{r}
ggplot(top.1000.sites, aes(x = log(PageViews), y = log(UniqueVisitors))) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```

회귀 분석을 돌려보고 summary() 함수를 이용해 분석 결과를 요약해보자.
```{r}
lm.fit <- lm(log(PageViews) ~ log(UniqueVisitors),
             data = top.1000.sites)

summary(lm.fit)
```

출력값을 자세히 살펴보자.

* lm 호출 내용
* 잔차의 사분위수 -> 요약 수치보다는 그래프에서 더 많은 정보를 얻을 수 있다. 
* 회귀계수 
    + Estimate(추정) 열 : coef()가 출력하는 값
    + Std. Error(표준오차) : 신뢰성 측도
    + t value, p-value : 참 계수가 0이 아닐 확신의 정도를 측정하는 값들이며 조사중인 입력과 출력 사이에 실제로 연관관계가 있는지 정할 때 사용된다. t 값의 의미는 계수와 표준오차의 배수를 계산하여 구한다. 계수가 0으로부터 2 표준오차만큼 떨어져 있는지 여부를 가지고 유의성 여부를 판단한다. 이는 예전부터 입출력 간의 상관관계 여부를 걸정하는 경계값(cutoff)으로 쓰였다. 
* 유의성 코드 
* Residual standard error(잔차 표준 오차) : 
    + 예측검증력(predictive power)과 관련된 값으로 sqrt(mean(residuals(lm.fit)^2))로 계산되는 RMSE 값이다. 
    + 자유도, 과대적합(overfitting) : 두 개의 모수 알파와 베타를 추정해야 하므로 n-2의 자유도를 가진다 -> 6장에서 자세히 다룬다. 
* R-squared, Adjusted R-squared : 모형이 데이터 분산의 몇 %를 설명하는지 나타내는 값.
* F 통계량 : 평균 예측치에 대비한 모형의 성능 개선 측도. p값은 계산 원리를 완전히 이해해야만 쓸모가 있으며, 그렇지 않을 경우 잘못된 정보에 기반하여 잘못된 결론을 내릴 수도 있다. -> 6장에서 자세히 다룬다. 

HasAdvertising, IsEnglish를 모형 입력에 포함시켜 분석해보자.
```{r}
lm.fit <- lm(log(PageViews) ~ HasAdvertising + log(UniqueVisitors) + InEnglish,
             data = top.1000.sites)
summary(lm.fit)
```

중회귀 분석의 수행 결과는 기본적으로 단순선형회귀분석과 동일하게 해석하면 된다.  

여기서 주의할 점은 factor형 변수를 추가한 것이다.  
회귀모형에서 factor를 사용할 때는 한 레벨을 절편의 일부로, 다른 레벨은 모형에 직접 포함되도록 해야 한다.  
HasAdvertising==No인 웹사이트는 절편에 포함되도록 모형화함.  
바꿔말하면, 광고가 없는 어떤 웹사이트의 UniqueVisitors가 1명일 때의 예측치가 절편값이라는 의미

세 입력을 따로 써서 어떤 입력값이 가장 예측력이 좋은지 비교해보자.
```{r}
lm.fit <- lm(log(PageViews) ~ HasAdvertising,
             data = top.1000.sites)
summary(lm.fit)$r.squared

lm.fit <- lm(log(PageViews) ~ log(UniqueVisitors),
             data = top.1000.sites)
summary(lm.fit)$r.squared

lm.fit <- lm(log(PageViews) ~ InEnglish,
             data = top.1000.sites)
summary(lm.fit)$r.squared
```

각변수가 각각 분산의 1%, 46%, 3%를 설명한다.  


#### 상관계수 정의

상관계수 : 두 변수 사이의 두 변수 사이의 직선관계의 강도를 나타냄

x와 y를 그려보면 완벽한 선형 관계는 아니다. 
```{r}
x <- 1:10
y <- x ^ 2

ggplot(data.frame(X = x, Y = y), aes(x = X, y = Y)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```

그러면 얼마나 선형에 가까울까? cor() 함수로 상관계수를 구하면 된다. 
```{r}
cor(x, y)
```

cor() 함수 대신 직접 lm을 활용해서 구해볼까? 먼저 x와 y의 척도를 맞춰주고(scale 함수 사용) 회귀 분석을 수행하면 x와 y 사이 상관계수가 척도 조정된 두 변수의 선형 회귀 계수와 거의 같은 것을 발견할 수 있다. 
```{r}
coef(lm(scale(y) ~ scale(x)))
```

상관계수는 두 변수의 관계가 얼마나 선형적인지를 나타내는 측도이기 때문에  
인과 관계에 대해서는 알려주는 바가 전혀 없다. => "Correlation does not imply causation"
![Correlation Is Not Causation](images/correlation.png)
